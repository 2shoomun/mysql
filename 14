# import the libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style("darkgrid")  # darkgrid, whitegrid, dark, white, and ticks
%matplotlib inline

import string
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.feature_extraction.text import TfidfVectorizer

# LOAD THE DATASET
messages = pd.read_csv('spam.csv', encoding='latin-1')
messages.head()

# Remove the unnecessary columns for dataset and rename the column names.
messages = messages.drop(labels=["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"], axis=1)
messages.columns = ["label", "message"]

# Exploratory Data Analysis
# Let's check out some of the stats with some plots and the built-in methods in pandas!
messages.info()
messages.describe()
messages.groupby('label').describe().T
messages["label"].value_counts().plot(kind='pie', explode=[0, 0.1], figsize=(6, 6), autopct='%1.1f%%', shadow=True)
plt.title("Spam vs Ham")
plt.legend(["Ham", "Spam"])
plt.show()

# Text Pre-processing
def text_preprocess(mess):
    """
    Takes in a string of text, then performs the following:
    1. Remove all punctuation
    2. Remove all stopwords
    3. Returns a list of the cleaned text
    """
    # Check characters to see if they are in punctuation
    nopunc = [char for char in mess if char not in string.punctuation]
    # Join the characters again to form the string.
    nopunc = ''.join(nopunc)
    nopunc = nopunc.lower()
    # Now just remove any stopwords and non-alphabets
    nostop = [word for word in nopunc.split() if word.lower() not in stopwords.words('english') and word.isalpha()]
    return nostop

spam_messages = messages[messages["label"] == "spam"]["message"]
ham_messages = messages[messages["label"] == "ham"]["message"]
print("No of spam messages : ", len(spam_messages))
print("No of ham messages : ", len(ham_messages))

# Wordcloud for Spam Messages
spam_words = text_preprocess(spam_messages)
spam_words[:10]
spam_wordcloud = WordCloud(width=600, height=400).generate(' '.join(spam_words))
plt.figure(figsize=(10, 8), facecolor='k')
plt.imshow(spam_wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

print("Top 10 Spam words are :\n")
print(pd.Series(spam_words).value_counts().head(10))

# Wordcloud for Ham Messages
ham_words = text_preprocess(ham_messages)
ham_wordcloud = WordCloud(width=600, height=400).generate(' '.join(ham_words))
plt.figure(figsize=(10, 8), facecolor='k')
plt.imshow(ham_wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

print("Top 10 Ham words are :\n")
print(pd.Series(ham_words).value_counts().head(10))

# Data Transformation
# Let's remove punctuations/ stopwords from all SMS
messages["message"] = messages["message"].apply(text_preprocess)
# Convert the SMS into a string from a list
messages["message"] = messages["message"].agg(lambda x: ' '.join(map(str, x)))

# Creating the Bag of Words
vectorizer = CountVectorizer()
bow_transformer = vectorizer.fit(messages['message'])

print("20 Bag of Words (BOW) Features: \n")
print("\nTotal number of vocab words: ", len(vectorizer.vocabulary_))

message4 = messages['message'][3]

# fit_transform: Learn the vocabulary dictionary and return term-document matrix.
bow4 = bow_transformer.transform([message4])
print(bow4)
print(bow4.shape)

messages_bow = bow_transformer.transform(messages['message'])

print('Shape of Sparse Matrix: ', messages_bow.shape)
print('Amount of Non-Zero occurrences: ', messages_bow.nnz)

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer().fit(messages_bow)

tfidf4 = tfidf_transformer.transform(bow4)
print(tfidf4)

vec = TfidfVectorizer(encoding="latin-1", strip_accents="unicode", stop_words="english")
features = vec.fit_transform(messages["message"])
print(features.shape)

print(len(vec.vocabulary_))
print(tfidf_transformer.idf_[bow_transformer.vocabulary_['say']])

messages_tfidf = tfidf_transformer.transform(messages_bow)
print(messages_tfidf.shape)

# Model Evaluation
# Train Test Split
msg_train, msg_test, label_train, label_test = train_test_split(messages_tfidf, messages['label'], test_size=0.2)

print("train dataset features size : ", msg_train.shape)
print("train dataset label size", label_train.shape)
print("\n")
print("test dataset features size", msg_test.shape)
print("test dataset label size", label_test.shape)

# Building Naive Bayes classifier Model
from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB()
spam_detect_model = clf.fit(msg_train, label_train)
predict_train = spam_detect_model.predict(msg_train)

print("Classification Report \n", metrics.classification_report(label_train, predict_train))
print("\n")
print("Confusion Matrix \n", metrics.confusion_matrix(label_train, predict_train))
print("\n")
print("Accuracy of Train dataset : {0:0.3f}".format(metrics.accuracy_score(label_train, predict_train)))

# Model Evaluation
label_predictions = spam_detect_model.predict(msg_test)
print(label_predictions)
print('predicted:', spam_detect_model.predict(tfidf4)[0])
print('expected:', messages['label'][3])

# Printing the Overall Accuracy of the model
print("Accuracy of the model : {0:0.3f}".format(metrics.accuracy_score(label_test, label_predictions)))
